{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8074bcd9",
   "metadata": {},
   "source": [
    "##### T1-1 이상치(IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bea867",
   "metadata": {},
   "source": [
    "- 데이터에서 IQR을 활용해 Fare컬럼의 이상치를 찾고, 이상치 데이터의 여성 수를 구하시오\n",
    "- titanic/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c87cea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titanic_train = pd.read_csv('../data/titanic/train.csv')\n",
    "titanic_train.head()\n",
    "\n",
    "dir(pd)\n",
    "dir(np)\n",
    "\n",
    "lower_quantile = titanic_train['Fare'].quantile(0.25) # Q1\n",
    "upper_quantile = titanic_train['Fare'].quantile(0.75) # Q3\n",
    "iqr = upper_quantile - lower_quantile\n",
    "iqr\n",
    "\n",
    "lower_bound = lower_quantile - (1.5 * iqr) \n",
    "upper_bound = upper_quantile + (1.5 * iqr)\n",
    "\n",
    "outlier = titanic_train.loc[(titanic_train['Fare'] < lower_bound)|(titanic_train['Fare'] > upper_bound)]['Sex']\n",
    "outlier.head()\n",
    "\n",
    "print(outlier[outlier == 'female'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35311051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-26.724, 65.6344)\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "# 1-1 sol)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('../data/titanic/train.csv')\n",
    "\n",
    "# print(df.isnull().sum())\n",
    "# df.head()\n",
    "\n",
    "Q1 = df['Fare'].quantile(0.25)\n",
    "Q3 = df['Fare'].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "\n",
    "fare_outlier = df.loc[(df['Fare'] < lower_bound) | (df['Fare'] > upper_bound)]\n",
    "\n",
    "print((f\"({lower_bound}, {upper_bound})\"))\n",
    "print(sum(fare_outlier['Sex'] == 'female'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167bb5e",
   "metadata": {},
   "source": [
    "##### T1-2 이상치(소수점 나이)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6437771",
   "metadata": {},
   "source": [
    "- 주어진 데이터에서 이상치(소수점 나이)를 찾고 올림, 내림, 버림(절사)했을때 3가지 모두 이상치 'age' 평균을 구한 다음 모두 더하여 출력하시오\n",
    "- basic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "826eca2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['False_',\n",
       " 'ScalarType',\n",
       " 'True_',\n",
       " '_CopyMode',\n",
       " '_NoValue',\n",
       " '__NUMPY_SETUP__',\n",
       " '__all__',\n",
       " '__array_api_version__',\n",
       " '__array_namespace_info__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__config__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__expired_attributes__',\n",
       " '__file__',\n",
       " '__former_attrs__',\n",
       " '__future_scalars__',\n",
       " '__getattr__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numpy_submodules__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_array_api_info',\n",
       " '_core',\n",
       " '_distributor_init',\n",
       " '_expired_attrs_2_0',\n",
       " '_globals',\n",
       " '_int_extended_msg',\n",
       " '_mat',\n",
       " '_msg',\n",
       " '_pyinstaller_hooks_dir',\n",
       " '_pytesttester',\n",
       " '_specific_msg',\n",
       " '_type_info',\n",
       " '_typing',\n",
       " '_utils',\n",
       " 'abs',\n",
       " 'absolute',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply_along_axis',\n",
       " 'apply_over_axes',\n",
       " 'arange',\n",
       " 'arccos',\n",
       " 'arccosh',\n",
       " 'arcsin',\n",
       " 'arcsinh',\n",
       " 'arctan',\n",
       " 'arctan2',\n",
       " 'arctanh',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argpartition',\n",
       " 'argsort',\n",
       " 'argwhere',\n",
       " 'around',\n",
       " 'array',\n",
       " 'array2string',\n",
       " 'array_equal',\n",
       " 'array_equiv',\n",
       " 'array_repr',\n",
       " 'array_split',\n",
       " 'array_str',\n",
       " 'asanyarray',\n",
       " 'asarray',\n",
       " 'asarray_chkfinite',\n",
       " 'ascontiguousarray',\n",
       " 'asfortranarray',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'asmatrix',\n",
       " 'astype',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'atleast_1d',\n",
       " 'atleast_2d',\n",
       " 'atleast_3d',\n",
       " 'average',\n",
       " 'bartlett',\n",
       " 'base_repr',\n",
       " 'binary_repr',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_count',\n",
       " 'bitwise_invert',\n",
       " 'bitwise_left_shift',\n",
       " 'bitwise_not',\n",
       " 'bitwise_or',\n",
       " 'bitwise_right_shift',\n",
       " 'bitwise_xor',\n",
       " 'blackman',\n",
       " 'block',\n",
       " 'bmat',\n",
       " 'bool',\n",
       " 'bool_',\n",
       " 'broadcast',\n",
       " 'broadcast_arrays',\n",
       " 'broadcast_shapes',\n",
       " 'broadcast_to',\n",
       " 'busday_count',\n",
       " 'busday_offset',\n",
       " 'busdaycalendar',\n",
       " 'byte',\n",
       " 'bytes_',\n",
       " 'c_',\n",
       " 'can_cast',\n",
       " 'cbrt',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'char',\n",
       " 'character',\n",
       " 'choose',\n",
       " 'clip',\n",
       " 'clongdouble',\n",
       " 'column_stack',\n",
       " 'common_type',\n",
       " 'complex128',\n",
       " 'complex64',\n",
       " 'complexfloating',\n",
       " 'compress',\n",
       " 'concat',\n",
       " 'concatenate',\n",
       " 'conj',\n",
       " 'conjugate',\n",
       " 'convolve',\n",
       " 'copy',\n",
       " 'copysign',\n",
       " 'copyto',\n",
       " 'core',\n",
       " 'corrcoef',\n",
       " 'correlate',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cross',\n",
       " 'csingle',\n",
       " 'ctypeslib',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'cumulative_prod',\n",
       " 'cumulative_sum',\n",
       " 'datetime64',\n",
       " 'datetime_as_string',\n",
       " 'datetime_data',\n",
       " 'deg2rad',\n",
       " 'degrees',\n",
       " 'delete',\n",
       " 'diag',\n",
       " 'diag_indices',\n",
       " 'diag_indices_from',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diff',\n",
       " 'digitize',\n",
       " 'divide',\n",
       " 'divmod',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dstack',\n",
       " 'dtype',\n",
       " 'dtypes',\n",
       " 'e',\n",
       " 'ediff1d',\n",
       " 'einsum',\n",
       " 'einsum_path',\n",
       " 'emath',\n",
       " 'empty',\n",
       " 'empty_like',\n",
       " 'equal',\n",
       " 'errstate',\n",
       " 'euler_gamma',\n",
       " 'exceptions',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'expand_dims',\n",
       " 'expm1',\n",
       " 'extract',\n",
       " 'eye',\n",
       " 'f2py',\n",
       " 'fabs',\n",
       " 'fft',\n",
       " 'fill_diagonal',\n",
       " 'finfo',\n",
       " 'fix',\n",
       " 'flatiter',\n",
       " 'flatnonzero',\n",
       " 'flexible',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float16',\n",
       " 'float32',\n",
       " 'float64',\n",
       " 'float_power',\n",
       " 'floating',\n",
       " 'floor',\n",
       " 'floor_divide',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'format_float_positional',\n",
       " 'format_float_scientific',\n",
       " 'frexp',\n",
       " 'from_dlpack',\n",
       " 'frombuffer',\n",
       " 'fromfile',\n",
       " 'fromfunction',\n",
       " 'fromiter',\n",
       " 'frompyfunc',\n",
       " 'fromregex',\n",
       " 'fromstring',\n",
       " 'full',\n",
       " 'full_like',\n",
       " 'gcd',\n",
       " 'generic',\n",
       " 'genfromtxt',\n",
       " 'geomspace',\n",
       " 'get_include',\n",
       " 'get_printoptions',\n",
       " 'getbufsize',\n",
       " 'geterr',\n",
       " 'geterrcall',\n",
       " 'gradient',\n",
       " 'greater',\n",
       " 'greater_equal',\n",
       " 'half',\n",
       " 'hamming',\n",
       " 'hanning',\n",
       " 'heaviside',\n",
       " 'histogram',\n",
       " 'histogram2d',\n",
       " 'histogram_bin_edges',\n",
       " 'histogramdd',\n",
       " 'hsplit',\n",
       " 'hstack',\n",
       " 'hypot',\n",
       " 'i0',\n",
       " 'identity',\n",
       " 'iinfo',\n",
       " 'imag',\n",
       " 'in1d',\n",
       " 'index_exp',\n",
       " 'indices',\n",
       " 'inexact',\n",
       " 'inf',\n",
       " 'info',\n",
       " 'inner',\n",
       " 'insert',\n",
       " 'int16',\n",
       " 'int32',\n",
       " 'int64',\n",
       " 'int8',\n",
       " 'int_',\n",
       " 'intc',\n",
       " 'integer',\n",
       " 'interp',\n",
       " 'intersect1d',\n",
       " 'intp',\n",
       " 'invert',\n",
       " 'is_busday',\n",
       " 'isclose',\n",
       " 'iscomplex',\n",
       " 'iscomplexobj',\n",
       " 'isdtype',\n",
       " 'isfinite',\n",
       " 'isfortran',\n",
       " 'isin',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isnat',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'isrealobj',\n",
       " 'isscalar',\n",
       " 'issubdtype',\n",
       " 'iterable',\n",
       " 'ix_',\n",
       " 'kaiser',\n",
       " 'kron',\n",
       " 'lcm',\n",
       " 'ldexp',\n",
       " 'left_shift',\n",
       " 'less',\n",
       " 'less_equal',\n",
       " 'lexsort',\n",
       " 'lib',\n",
       " 'linalg',\n",
       " 'linspace',\n",
       " 'little_endian',\n",
       " 'load',\n",
       " 'loadtxt',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logical_and',\n",
       " 'logical_not',\n",
       " 'logical_or',\n",
       " 'logical_xor',\n",
       " 'logspace',\n",
       " 'long',\n",
       " 'longdouble',\n",
       " 'longlong',\n",
       " 'ma',\n",
       " 'mask_indices',\n",
       " 'matmul',\n",
       " 'matrix',\n",
       " 'matrix_transpose',\n",
       " 'matvec',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'may_share_memory',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'memmap',\n",
       " 'meshgrid',\n",
       " 'mgrid',\n",
       " 'min',\n",
       " 'min_scalar_type',\n",
       " 'minimum',\n",
       " 'mintypecode',\n",
       " 'mod',\n",
       " 'modf',\n",
       " 'moveaxis',\n",
       " 'multiply',\n",
       " 'nan',\n",
       " 'nan_to_num',\n",
       " 'nanargmax',\n",
       " 'nanargmin',\n",
       " 'nancumprod',\n",
       " 'nancumsum',\n",
       " 'nanmax',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanmin',\n",
       " 'nanpercentile',\n",
       " 'nanprod',\n",
       " 'nanquantile',\n",
       " 'nanstd',\n",
       " 'nansum',\n",
       " 'nanvar',\n",
       " 'ndarray',\n",
       " 'ndenumerate',\n",
       " 'ndim',\n",
       " 'ndindex',\n",
       " 'nditer',\n",
       " 'negative',\n",
       " 'nested_iters',\n",
       " 'newaxis',\n",
       " 'nextafter',\n",
       " 'nonzero',\n",
       " 'not_equal',\n",
       " 'number',\n",
       " 'object_',\n",
       " 'ogrid',\n",
       " 'ones',\n",
       " 'ones_like',\n",
       " 'outer',\n",
       " 'packbits',\n",
       " 'pad',\n",
       " 'partition',\n",
       " 'percentile',\n",
       " 'permute_dims',\n",
       " 'pi',\n",
       " 'piecewise',\n",
       " 'place',\n",
       " 'poly',\n",
       " 'poly1d',\n",
       " 'polyadd',\n",
       " 'polyder',\n",
       " 'polydiv',\n",
       " 'polyfit',\n",
       " 'polyint',\n",
       " 'polymul',\n",
       " 'polynomial',\n",
       " 'polysub',\n",
       " 'polyval',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'power',\n",
       " 'printoptions',\n",
       " 'prod',\n",
       " 'promote_types',\n",
       " 'ptp',\n",
       " 'put',\n",
       " 'put_along_axis',\n",
       " 'putmask',\n",
       " 'quantile',\n",
       " 'r_',\n",
       " 'rad2deg',\n",
       " 'radians',\n",
       " 'random',\n",
       " 'ravel',\n",
       " 'ravel_multi_index',\n",
       " 'real',\n",
       " 'real_if_close',\n",
       " 'rec',\n",
       " 'recarray',\n",
       " 'reciprocal',\n",
       " 'record',\n",
       " 'remainder',\n",
       " 'repeat',\n",
       " 'require',\n",
       " 'reshape',\n",
       " 'resize',\n",
       " 'result_type',\n",
       " 'right_shift',\n",
       " 'rint',\n",
       " 'roll',\n",
       " 'rollaxis',\n",
       " 'roots',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'row_stack',\n",
       " 's_',\n",
       " 'save',\n",
       " 'savetxt',\n",
       " 'savez',\n",
       " 'savez_compressed',\n",
       " 'sctypeDict',\n",
       " 'searchsorted',\n",
       " 'select',\n",
       " 'set_printoptions',\n",
       " 'setbufsize',\n",
       " 'setdiff1d',\n",
       " 'seterr',\n",
       " 'seterrcall',\n",
       " 'setxor1d',\n",
       " 'shape',\n",
       " 'shares_memory',\n",
       " 'short',\n",
       " 'show_config',\n",
       " 'show_runtime',\n",
       " 'sign',\n",
       " 'signbit',\n",
       " 'signedinteger',\n",
       " 'sin',\n",
       " 'sinc',\n",
       " 'single',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'sort',\n",
       " 'sort_complex',\n",
       " 'spacing',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'square',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'std',\n",
       " 'str_',\n",
       " 'strings',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'take',\n",
       " 'take_along_axis',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tensordot',\n",
       " 'test',\n",
       " 'testing',\n",
       " 'tile',\n",
       " 'timedelta64',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'trapezoid',\n",
       " 'trapz',\n",
       " 'tri',\n",
       " 'tril',\n",
       " 'tril_indices',\n",
       " 'tril_indices_from',\n",
       " 'trim_zeros',\n",
       " 'triu',\n",
       " 'triu_indices',\n",
       " 'triu_indices_from',\n",
       " 'true_divide',\n",
       " 'trunc',\n",
       " 'typecodes',\n",
       " 'typename',\n",
       " 'typing',\n",
       " 'ubyte',\n",
       " 'ufunc',\n",
       " 'uint',\n",
       " 'uint16',\n",
       " 'uint32',\n",
       " 'uint64',\n",
       " 'uint8',\n",
       " 'uintc',\n",
       " 'uintp',\n",
       " 'ulong',\n",
       " 'ulonglong',\n",
       " 'union1d',\n",
       " 'unique',\n",
       " 'unique_all',\n",
       " 'unique_counts',\n",
       " 'unique_inverse',\n",
       " 'unique_values',\n",
       " 'unpackbits',\n",
       " 'unravel_index',\n",
       " 'unsignedinteger',\n",
       " 'unstack',\n",
       " 'unwrap',\n",
       " 'ushort',\n",
       " 'vander',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'vecdot',\n",
       " 'vecmat',\n",
       " 'vectorize',\n",
       " 'void',\n",
       " 'vsplit',\n",
       " 'vstack',\n",
       " 'where',\n",
       " 'zeros',\n",
       " 'zeros_like']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "996f9510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LGCARE\\AppData\\Local\\Temp\\ipykernel_6340\\3823649341.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier['올림'] = np.ceil(outlier['age'])\n",
      "C:\\Users\\LGCARE\\AppData\\Local\\Temp\\ipykernel_6340\\3823649341.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier['내림'] = np.floor(outlier['age'])\n",
      "C:\\Users\\LGCARE\\AppData\\Local\\Temp\\ipykernel_6340\\3823649341.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlier['절사'] = np.trunc(outlier['age'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "basic = pd.read_csv('../data/basic1.csv')\n",
    "basic\n",
    "\n",
    "# 나이 소수점있는 데이터 찾기\n",
    "outlier = basic[(basic['age'] - np.floor(basic['age'])) != 0]\n",
    "outlier.head()\n",
    "\n",
    "outlier['올림'] = np.ceil(outlier['age'])\n",
    "outlier.head()\n",
    "\n",
    "outlier['내림'] = np.floor(outlier['age'])\n",
    "outlier.head()\n",
    "\n",
    "outlier['절사'] = np.trunc(outlier['age'])\n",
    "outlier.head()\n",
    "\n",
    "mean_1 = outlier['올림'].mean()\n",
    "mean_2 = outlier['내림'].mean() # -5.5 내림 > -6\n",
    "mean_3 = outlier['절사'].mean() # -5.5 절사 > -5\n",
    "\n",
    "print(mean_1 + mean_2 + mean_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c025da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.5\n"
     ]
    }
   ],
   "source": [
    "# 1-2 sol)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df.head()\n",
    "\n",
    "# 나이가 소숫점인 데이터\n",
    "df = df[(df['age'] - np.floor(df['age'])) != 0]\n",
    "df\n",
    "\n",
    "m_ceil = np.ceil(df['age']).mean()\n",
    "m_floor = np.floor(df['age']).mean()\n",
    "m_trunc = np.trunc(df['age']).mean()\n",
    "\n",
    "m_ceil, m_floor, m_trunc\n",
    "\n",
    "print(m_ceil + m_floor + m_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79218a",
   "metadata": {},
   "source": [
    "##### T1-3 결측치 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609132c3",
   "metadata": {},
   "source": [
    "- 주어진 데이터에서 결측치가 80%이상 되는 컬럼은(변수는) 삭제하고, 80% 미만인 결측치가 있는 컬럼은 'city'별 중앙값으로 값을 대체하고 'f1'컬럼의 평균값을 출력하세요!\n",
    "- basic1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81bf847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      100 non-null    object \n",
      " 1   age     100 non-null    float64\n",
      " 2   city    100 non-null    object \n",
      " 3   f1      69 non-null     float64\n",
      " 4   f2      100 non-null    int64  \n",
      " 5   f4      100 non-null    object \n",
      " 6   f5      100 non-null    float64\n",
      "dtypes: float64(3), int64(1), object(3)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "basic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fac7d46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function groupby in module pandas.core.frame:\n",
      "\n",
      "groupby(self, by=None, axis: 'Axis | lib.NoDefault' = <no_default>, level: 'IndexLabel | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, observed: 'bool | lib.NoDefault' = <no_default>, dropna: 'bool' = True) -> 'DataFrameGroupBy'\n",
      "    Group DataFrame using a mapper or by a Series of columns.\n",
      "    \n",
      "    A groupby operation involves some combination of splitting the\n",
      "    object, applying a function, and combining the results. This can be\n",
      "    used to group large amounts of data and compute operations on these\n",
      "    groups.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    by : mapping, function, label, pd.Grouper or list of such\n",
      "        Used to determine the groups for the groupby.\n",
      "        If ``by`` is a function, it's called on each value of the object's\n",
      "        index. If a dict or Series is passed, the Series or dict VALUES\n",
      "        will be used to determine the groups (the Series' values are first\n",
      "        aligned; see ``.align()`` method). If a list or ndarray of length\n",
      "        equal to the selected axis is passed (see the `groupby user guide\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups>`_),\n",
      "        the values are used as-is to determine the groups. A label or list\n",
      "        of labels may be passed to group by the columns in ``self``.\n",
      "        Notice that a tuple is interpreted as a (single) key.\n",
      "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "        Split along rows (0) or columns (1). For `Series` this parameter\n",
      "        is unused and defaults to 0.\n",
      "    \n",
      "        .. deprecated:: 2.1.0\n",
      "    \n",
      "            Will be removed and behave like axis=0 in a future version.\n",
      "            For ``axis=1``, do ``frame.T.groupby(...)`` instead.\n",
      "    \n",
      "    level : int, level name, or sequence of such, default None\n",
      "        If the axis is a MultiIndex (hierarchical), group by a particular\n",
      "        level or levels. Do not specify both ``by`` and ``level``.\n",
      "    as_index : bool, default True\n",
      "        Return object with group labels as the\n",
      "        index. Only relevant for DataFrame input. as_index=False is\n",
      "        effectively \"SQL-style\" grouped output. This argument has no effect\n",
      "        on filtrations (see the `filtrations in the user guide\n",
      "        <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration>`_),\n",
      "        such as ``head()``, ``tail()``, ``nth()`` and in transformations\n",
      "        (see the `transformations in the user guide\n",
      "        <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation>`_).\n",
      "    sort : bool, default True\n",
      "        Sort group keys. Get better performance by turning this off.\n",
      "        Note this does not influence the order of observations within each\n",
      "        group. Groupby preserves the order of rows within each group. If False,\n",
      "        the groups will appear in the same order as they did in the original DataFrame.\n",
      "        This argument has no effect on filtrations (see the `filtrations in the user guide\n",
      "        <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration>`_),\n",
      "        such as ``head()``, ``tail()``, ``nth()`` and in transformations\n",
      "        (see the `transformations in the user guide\n",
      "        <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation>`_).\n",
      "    \n",
      "        .. versionchanged:: 2.0.0\n",
      "    \n",
      "            Specifying ``sort=False`` with an ordered categorical grouper will no\n",
      "            longer sort the values.\n",
      "    \n",
      "    group_keys : bool, default True\n",
      "        When calling apply and the ``by`` argument produces a like-indexed\n",
      "        (i.e. :ref:`a transform <groupby.transform>`) result, add group keys to\n",
      "        index to identify pieces. By default group keys are not included\n",
      "        when the result's index (and column) labels match the inputs, and\n",
      "        are included otherwise.\n",
      "    \n",
      "        .. versionchanged:: 1.5.0\n",
      "    \n",
      "           Warns that ``group_keys`` will no longer be ignored when the\n",
      "           result from ``apply`` is a like-indexed Series or DataFrame.\n",
      "           Specify ``group_keys`` explicitly to include the group keys or\n",
      "           not.\n",
      "    \n",
      "        .. versionchanged:: 2.0.0\n",
      "    \n",
      "           ``group_keys`` now defaults to ``True``.\n",
      "    \n",
      "    observed : bool, default False\n",
      "        This only applies if any of the groupers are Categoricals.\n",
      "        If True: only show observed values for categorical groupers.\n",
      "        If False: show all values for categorical groupers.\n",
      "    \n",
      "        .. deprecated:: 2.1.0\n",
      "    \n",
      "            The default value will change to True in a future version of pandas.\n",
      "    \n",
      "    dropna : bool, default True\n",
      "        If True, and if group keys contain NA values, NA values together\n",
      "        with row/column will be dropped.\n",
      "        If False, NA values will also be treated as the key in groups.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    pandas.api.typing.DataFrameGroupBy\n",
      "        Returns a groupby object that contains information about the groups.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    resample : Convenience method for frequency conversion and resampling\n",
      "        of time series.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    See the `user guide\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/groupby.html>`__ for more\n",
      "    detailed usage and examples, including splitting an object into groups,\n",
      "    iterating through groups, selecting a group, aggregation, and more.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
      "    ...                               'Parrot', 'Parrot'],\n",
      "    ...                    'Max Speed': [380., 370., 24., 26.]})\n",
      "    >>> df\n",
      "       Animal  Max Speed\n",
      "    0  Falcon      380.0\n",
      "    1  Falcon      370.0\n",
      "    2  Parrot       24.0\n",
      "    3  Parrot       26.0\n",
      "    >>> df.groupby(['Animal']).mean()\n",
      "            Max Speed\n",
      "    Animal\n",
      "    Falcon      375.0\n",
      "    Parrot       25.0\n",
      "    \n",
      "    **Hierarchical Indexes**\n",
      "    \n",
      "    We can groupby different levels of a hierarchical index\n",
      "    using the `level` parameter:\n",
      "    \n",
      "    >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n",
      "    ...           ['Captive', 'Wild', 'Captive', 'Wild']]\n",
      "    >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n",
      "    >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n",
      "    ...                   index=index)\n",
      "    >>> df\n",
      "                    Max Speed\n",
      "    Animal Type\n",
      "    Falcon Captive      390.0\n",
      "           Wild         350.0\n",
      "    Parrot Captive       30.0\n",
      "           Wild          20.0\n",
      "    >>> df.groupby(level=0).mean()\n",
      "            Max Speed\n",
      "    Animal\n",
      "    Falcon      370.0\n",
      "    Parrot       25.0\n",
      "    >>> df.groupby(level=\"Type\").mean()\n",
      "             Max Speed\n",
      "    Type\n",
      "    Captive      210.0\n",
      "    Wild         185.0\n",
      "    \n",
      "    We can also choose to include NA in group keys or not by setting\n",
      "    `dropna` parameter, the default setting is `True`.\n",
      "    \n",
      "    >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n",
      "    >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n",
      "    \n",
      "    >>> df.groupby(by=[\"b\"]).sum()\n",
      "        a   c\n",
      "    b\n",
      "    1.0 2   3\n",
      "    2.0 2   5\n",
      "    \n",
      "    >>> df.groupby(by=[\"b\"], dropna=False).sum()\n",
      "        a   c\n",
      "    b\n",
      "    1.0 2   3\n",
      "    2.0 2   5\n",
      "    NaN 1   4\n",
      "    \n",
      "    >>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n",
      "    >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n",
      "    \n",
      "    >>> df.groupby(by=\"a\").sum()\n",
      "        b     c\n",
      "    a\n",
      "    a   13.0   13.0\n",
      "    b   12.3  123.0\n",
      "    \n",
      "    >>> df.groupby(by=\"a\", dropna=False).sum()\n",
      "        b     c\n",
      "    a\n",
      "    a   13.0   13.0\n",
      "    b   12.3  123.0\n",
      "    NaN 12.3   33.0\n",
      "    \n",
      "    When using ``.apply()``, use ``group_keys`` to include or exclude the\n",
      "    group keys. The ``group_keys`` argument defaults to ``True`` (include).\n",
      "    \n",
      "    >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
      "    ...                               'Parrot', 'Parrot'],\n",
      "    ...                    'Max Speed': [380., 370., 24., 26.]})\n",
      "    >>> df.groupby(\"Animal\", group_keys=True)[['Max Speed']].apply(lambda x: x)\n",
      "              Max Speed\n",
      "    Animal\n",
      "    Falcon 0      380.0\n",
      "           1      370.0\n",
      "    Parrot 2       24.0\n",
      "           3       26.0\n",
      "    \n",
      "    >>> df.groupby(\"Animal\", group_keys=False)[['Max Speed']].apply(lambda x: x)\n",
      "       Max Speed\n",
      "    0      380.0\n",
      "    1      370.0\n",
      "    2       24.0\n",
      "    3       26.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dir(basic)\n",
    "help(pd.DataFrame.groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f804827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# dir(sklearn.preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic.head()\n",
    "# dir(np)\n",
    "basic.isnull().sum() >= len(basic) * 0.8\n",
    "basic.drop('f3', axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5737d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(np.float64)\n",
    "# help(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613424e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        f1\n",
      "city      \n",
      "경기    58.0\n",
      "대구    75.0\n",
      "부산    62.0\n",
      "서울    68.0\n",
      "65.52\n"
     ]
    }
   ],
   "source": [
    "basic.isnull().sum() # f1 : 31\n",
    "\n",
    "\n",
    "city_f1_median = basic[['city', 'f1']].groupby(by = 'city').median()\n",
    "print(city_f1_median)\n",
    "# city_f1_median.index\n",
    "# city_f1_median.loc['서울']['f1']\n",
    "\n",
    "missing_data = basic[basic['f1'].isnull()]\n",
    "missing_data.head()\n",
    "\n",
    "for i in missing_data.index :\n",
    "  city = basic.loc[i, 'city']\n",
    "  basic.loc[i, 'f1'] = city_f1_median.loc[city]['f1']\n",
    "\n",
    "# 잘 들어갔는지 확인\n",
    "missing_data = basic[basic['f1'].isnull()]\n",
    "missing_data.head()\n",
    "\n",
    "print(basic['f1'].mean())\n",
    "\n",
    "# 솔루션대로\n",
    "# basic['f1'] = basic['f1'].fillna(df['city'].map({'서울' : city_f1_median['서울']['f1']})) # inplace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6fe5015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제 전 :  (100, 8)\n",
      "삭제 후 :  (100, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(65.52)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-3. sol)\n",
    "# pandas, numpy\n",
    "\n",
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df.head()\n",
    "\n",
    "df.isnull().sum()\n",
    "df.shape\n",
    "\n",
    "df.isnull().sum() / df.shape[0]\n",
    "\n",
    "# f3 컬럼 삭제\n",
    "print('삭제 전 : ', df.shape)\n",
    "df = df.drop(['f3'], axis= 1)\n",
    "print('삭제 후 : ', df.shape)\n",
    "\n",
    "df['city'].unique()\n",
    "\n",
    "seoul_median = df[df['city'] == '서울']['f1'].median()\n",
    "kyeonggi_median = df[df['city'] == '경기']['f1'].median()\n",
    "busan_median = df[df['city'] == '부산']['f1'].median()\n",
    "daegu_median = df[df['city'] == '대구']['f1'].median()\n",
    "\n",
    "# 방법 2\n",
    "# k, d, b, s = df.groupby('city')['f1'].median()\n",
    "seoul_median, kyeonggi_median, busan_median, daegu_median\n",
    "\n",
    "df['f1'] = df['f1'].fillna(df['city'].map({'서울' : seoul_median,\n",
    "                                            '경기' : kyeonggi_median,\n",
    "                                            '부산' : busan_median,\n",
    "                                            '대구' : daegu_median}))\n",
    "\n",
    "df['f1'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5681a9",
   "metadata": {},
   "source": [
    "##### T1-4 왜도/첨도(로그스케일)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a0a88",
   "metadata": {},
   "source": [
    "- 주어진 데이터 중 train.csv에서 'SalePrice'컬럼의 왜도와 첨도를 구한 값과, 'SalePrice'컬럼을 스케일링(log1p)로 변환한 이후 왜도와 첨도를 구해 모두 더한 다음 소수점 2째자리까지 출력하시오\n",
    "- houseprices / train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1cddd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(np)\n",
    "import sklearn\n",
    "# dir(sklearn)\n",
    "# import statistics\n",
    "# dir(statistics)\n",
    "import math\n",
    "# dir(math) # log1p\n",
    "# dir(pd.Series)\n",
    "# dir(sklearn.preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0b03f0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.35\n"
     ]
    }
   ],
   "source": [
    "# Series : skew, kurt # kurtosis?\n",
    "df = pd.read_csv('../data/houseprices/train.csv')\n",
    "df.head()\n",
    "\n",
    "saleprice_skew = df['SalePrice'].skew()\n",
    "saleprice_kurt = df['SalePrice'].kurt()\n",
    "\n",
    "df['log_SalePrice'] = df['SalePrice'].apply(math.log1p)\n",
    "log_saleprice_skew = df['log_SalePrice'].skew()\n",
    "log_saleprice_kurt = df['log_SalePrice'].kurt()\n",
    "\n",
    "total = saleprice_skew + saleprice_kurt + log_saleprice_skew + log_saleprice_kurt\n",
    "total_2f = round(total, 2)\n",
    "print(total_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0dc00636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.35\n"
     ]
    }
   ],
   "source": [
    "# 1-4. Sol)\n",
    "# df['SalePrice'].hist()\n",
    "\n",
    "s1 = df['SalePrice'].skew()\n",
    "k1 = df['SalePrice'].kurt()\n",
    "\n",
    "df['SalePrice_log'] = np.log1p(df['SalePrice'])\n",
    "# df['SalePrice_log'].hist()\n",
    "\n",
    "s2 = df['SalePrice_log'].skew()\n",
    "k2 = df['SalePrice_log'].kurt()\n",
    "\n",
    "print(round(s1+s2+k1+k2,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed400c9",
   "metadata": {},
   "source": [
    "##### T1-5 표준편차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231bbbf",
   "metadata": {},
   "source": [
    "- 주어진 데이터 중 basic1.csv에서 'f4'컬럼 값이 'ENFJ'와 'INFP'인 'f1'의 표준편차 차이를 절대값으로 구하시오\n",
    "- basic1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.859621525876811\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df.head()\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "enfj_std = df[df['f4'] == 'ENFJ']['f1'].std()\n",
    "infp_std = df[df['f4'] == 'INFP']['f1'].std()\n",
    "\n",
    "print(abs(enfj_std - infp_std))\n",
    "\n",
    "# sol와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b67542",
   "metadata": {},
   "source": [
    "##### T1-6 Groupby 합계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04406cc3",
   "metadata": {},
   "source": [
    "- 주어진 데이터 중 basic1.csv에서 'f1'컬럼 결측 데이터를 제거하고, 'city'와 'f2'을 기준으로 묶어 합계를 구하고, 'city가 경기이면서 f2가 0'인 조건에 만족하는 f1 값을 구하시오\n",
    "- basic1.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5151a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df.isnull().sum()\n",
    "\n",
    "drop_df = df[~df['f1'].isnull()]\n",
    "drop_df.isnull().sum()\n",
    "\n",
    "grouped_df = drop_df.groupby(['city', 'f2'])['f1'].sum()\n",
    "print(grouped_df['경기'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "71b51f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833.0\n"
     ]
    }
   ],
   "source": [
    "# 1-6 sol)\n",
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df = df.dropna(subset = ['f1'])\n",
    "\n",
    "df2 = df.groupby(['city', 'f2'])[['age', 'f1', 'f5']].sum()\n",
    "df2\n",
    "\n",
    "print(df2.iloc[0]['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18287b26",
   "metadata": {},
   "source": [
    "##### T1-7 값 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72868d25",
   "metadata": {},
   "source": [
    "- 'f4'컬럼의 값이 'ESFJ'인 데이터를 'ISFJ'로 대체하고, 'city'가 '경기'이면서 'f4'가 'ISFJ'인 데이터 중 'age'컬럼의 최대값을 출력하시오!\n",
    "- basic1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e05f5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(df['f4'])\n",
    "# help(pd.DataFrame.replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dcf83c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LGCARE\\AppData\\Local\\Temp\\ipykernel_14564\\558857038.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['f4'].replace(to_replace= {'ESFJ' : 'ISFJ'}, inplace= True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df.head(10)\n",
    "\n",
    "df['f4'].replace(to_replace= {'ESFJ' : 'ISFJ'}, inplace= True)\n",
    "# df.head(10)\n",
    "\n",
    "print(int(df[(df['city'] == '경기')&(df['f4'] == 'ISFJ')]['age'].max()))\n",
    "\n",
    "# 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "109b7983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(90.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T1-7 SOL)\n",
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df[df['f4'] == 'ESFJ']\n",
    "df['f4'] = df['f4'].replace('ESFJ', 'ISFJ')\n",
    "\n",
    "df[df['f4'] == 'ESFJ']\n",
    "df[(df['f4'] == 'ISFJ') & (df['city'] == '경기')]['age'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ee14f",
   "metadata": {},
   "source": [
    "##### T1-8 누적합/보간"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282aac07",
   "metadata": {},
   "source": [
    "- 주어진 데이터 셋에서 'f2' 컬럼이 1인 조건에 해당하는 데이터의 'f1'컬럼 누적합을 계산한다. 이때 발생하는 누적합 결측치는 바로 뒤의 값을 채우고, 누적합의 평균값을 출력한다. (단, 결측치 바로 뒤의 값이 없으면 다음에 나오는 값을 채워넣는다)\n",
    "- basic1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afaf9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(pd.Series) # cumsum\n",
    "# help(pd.Series.cumsum) # self, axis, skipna\n",
    "# dir(pd.Series) # bfill, backfill\n",
    "# help(pd.Series.bfill) # self, axis, inplace, limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1315.2702702702702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LGCARE\\AppData\\Local\\Temp\\ipykernel_14564\\994003479.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_f2['f1'] = df_f2['f1'].bfill()\n",
      "C:\\Users\\LGCARE\\AppData\\Local\\Temp\\ipykernel_14564\\994003479.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_f2['f1'] = df_f2['f1'].cumsum()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df.head()\n",
    "\n",
    "df_f2 = df[df['f2'] == 1]\n",
    "df_f2.head()\n",
    "\n",
    "# df_f2.isnull().sum() # f1 : 11\n",
    "df_f2['f1'] = df_f2['f1'].bfill()\n",
    "df_f2.isnull().sum()\n",
    "\n",
    "df_f2['f1'] = df_f2['f1'].cumsum()\n",
    "print(df_f2['f1'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03e6a712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980.3783783783783\n"
     ]
    }
   ],
   "source": [
    "# T1-8 sol)\n",
    "\n",
    "# s.fillna(method='bfill') # 바로 뒤값으로 채워넣는 방법\n",
    "# s.fillna(method='pad') # 이전값으로 채워넣는 방법\n",
    "df = pd.read_csv('../data/basic1.csv')\n",
    "df2 = df[df['f2']==1]['f1'].cumsum()\n",
    "df2.head(10)\n",
    "\n",
    "# df2 = df2.fillna(method= 'bfill')\n",
    "df2 = df2.bfill()\n",
    "df2.head(10)\n",
    "\n",
    "print(df2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecce2ec",
   "metadata": {},
   "source": [
    "##### T1-9 표준화(중앙값)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a9fc9",
   "metadata": {},
   "source": [
    "- 주어진 데이터에서 'f5'컬럼을 표준화(Standardization (Z-score Normalization))하고 그 중앙값을 구하시오¶\n",
    "- 데이터셋 : basic1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97006234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.260619629559015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/basic1.csv')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "f5_arr = np.array(df['f5']).reshape(-1, 1)\n",
    "\n",
    "df['f5'] = std_scaler.fit_transform(f5_arr)\n",
    "print(df['f5'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "851b566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.260619629559015\n"
     ]
    }
   ],
   "source": [
    "# T1-9 Sol)\n",
    "\n",
    "# 참고)\n",
    "# scaler = StandardScaler()\n",
    "# print(scaler.fit(data))\n",
    "# print(scaler.transform(data))\n",
    "\n",
    "df = pd.read_csv('../data/basic1.csv')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "df['f5'] = std_scaler.fit_transform(df[['f5']])\n",
    "print(df['f5'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49234f3a",
   "metadata": {},
   "source": [
    "##### T1-10 Yeo-Johnson & Box-Cox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb55f83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfb3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4399c22",
   "metadata": {},
   "source": [
    "##### T1-11 min-max scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924986f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae87e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "980ac87a",
   "metadata": {},
   "source": [
    "##### T1-12 상하위 10개 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e88da5",
   "metadata": {},
   "source": [
    "##### T1-13 상관관계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88ef6b",
   "metadata": {},
   "source": [
    "##### T1-14 Multilndex + Groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5eada",
   "metadata": {},
   "source": [
    "##### T1-15 조건 필터링 + 중앙값 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77f740",
   "metadata": {},
   "source": [
    "##### T1-16 분산 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22220b07",
   "metadata": {},
   "source": [
    "##### T1-17 시계열1(datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c8b87",
   "metadata": {},
   "source": [
    "##### T1-18 시계열2 (평일/주말)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9a4bd",
   "metadata": {},
   "source": [
    "##### T1-19 시계열3 (월별 총계)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9fc00",
   "metadata": {},
   "source": [
    "##### T1-20 데이터 병합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c2306",
   "metadata": {},
   "source": [
    "##### T1-21 Binning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f64959",
   "metadata": {},
   "source": [
    "##### T1-22 시계열(Weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0abcb",
   "metadata": {},
   "source": [
    "##### T1-23 중복 제거 및 결측치 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64f1cb",
   "metadata": {},
   "source": [
    "##### 기타)\n",
    "##### T1-24 Lagged Feature\n",
    "##### T1-25 문자열 처리(슬라이싱)\n",
    "##### T1-26 문자열 포함 여부\n",
    "##### T1-27 문자열 치환\n",
    "##### T1-28 빈도 계산(value_counts)\n",
    "##### T1-29 날짜 형식 변환\n",
    "##### T1-30 시계열 비율 계산\n",
    "##### T1-31 Melt 전체\n",
    "##### T1-32 Melt 일부\n",
    "##### T1-33 Timedelta 계산\n",
    "##### T1-34 패턴/선호도 분석 (난이도: 상)\n",
    "##### T1-35 피드백-문자와 시계열 (난이도: 중)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
